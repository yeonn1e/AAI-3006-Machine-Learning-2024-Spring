{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlnVsCY2UY0C"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "use_gpu = torch.cuda.is_available()\n",
        "if use_gpu:\n",
        "    print(\"Using CUDA\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = './data/ '\n",
        "TRAIN = 'train'\n",
        "VAL = 'val'\n",
        "TEST = 'test'\n",
        "\n",
        "# VGG-16 Takes 224x224 images as input, so we resize all of them\n",
        "data_transforms = {\n",
        "    TRAIN: transforms.Compose([\n",
        "        # Data augmentation is a good practice for the train set\n",
        "        # Here, we randomly crop the image to 224x224 and\n",
        "        # randomly flip it horizontally.\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "    ]),\n",
        "    VAL: transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "    ]),\n",
        "    TEST: transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "}\n",
        "\n",
        "image_datasets = {\n",
        "    x: datasets.ImageFolder(\n",
        "        os.path.join(data_dir, x),\n",
        "        transform=data_transforms[x]\n",
        "    )\n",
        "    for x in [TRAIN, VAL, TEST]\n",
        "}\n",
        "\n",
        "dataloaders = {\n",
        "    x: torch.utils.data.DataLoader(\n",
        "        image_datasets[x], batch_size=8,\n",
        "        shuffle=True, num_workers=4\n",
        "    )\n",
        "    for x in [TRAIN, VAL, TEST]\n",
        "}\n",
        "\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in [TRAIN, VAL, TEST]}\n",
        "\n",
        "for x in [TRAIN, VAL, TEST]:\n",
        "    print(\"Loaded {} images under {}\".format(dataset_sizes[x], x))\n",
        "\n",
        "print(\"Classes: \")\n",
        "class_names = image_datasets[TRAIN].classes\n",
        "print(image_datasets[TRAIN].classes)"
      ],
      "metadata": {
        "id": "OOtpQcutUdDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def imshow(inp, title=None):\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    # plt.figure(figsize=(10, 10))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)\n",
        "\n",
        "def show_databatch(inputs, classes):\n",
        "    out = torchvision.utils.make_grid(inputs)\n",
        "    imshow(out, title=[class_names[x] for x in classes])\n",
        "\n",
        "# Get a batch of training data\n",
        "inputs, classes = next(iter(dataloaders[TRAIN]))\n",
        "show_databatch(inputs, classes)"
      ],
      "metadata": {
        "id": "1eadBHzTVS-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_model(vgg, num_images=6):\n",
        "    was_training = vgg.training\n",
        "\n",
        "    # Set model for evaluation\n",
        "    vgg.train(False)\n",
        "    vgg.eval()\n",
        "\n",
        "    images_so_far = 0\n",
        "\n",
        "    for i, data in enumerate(dataloaders[TEST]):\n",
        "        inputs, labels = data\n",
        "        size = inputs.size()[0]\n",
        "\n",
        "        if use_gpu:\n",
        "            inputs, labels = Variable(inputs.cuda(), volatile=True), Variable(labels.cuda(), volatile=True)\n",
        "        else:\n",
        "            inputs, labels = Variable(inputs, volatile=True), Variable(labels, volatile=True)\n",
        "\n",
        "        outputs = vgg(inputs)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        predicted_labels = [preds[j] for j in range(inputs.size()[0])]\n",
        "\n",
        "        print(\"Ground truth:\")\n",
        "        show_databatch(inputs.data.cpu(), labels.data.cpu())\n",
        "        print(\"Prediction:\")\n",
        "        show_databatch(inputs.data.cpu(), predicted_labels)\n",
        "\n",
        "        del inputs, labels, outputs, preds, predicted_labels\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        images_so_far += size\n",
        "        if images_so_far >= num_images:\n",
        "            break\n",
        "\n",
        "    vgg.train(mode=was_training) # Revert model back to original training state"
      ],
      "metadata": {
        "id": "GyBIhkGvVVul"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}